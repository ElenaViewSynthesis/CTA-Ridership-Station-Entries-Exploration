{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 3 Starter Code\n",
    "\n",
    "You can use this notebook to answer the questions in quiz 3, on Decision Trees and Random forests. Some starter code and hints are also provided to keep you on the right track.\n",
    "\n",
    "Details are given in the quiz about the provenance of the data we're using. To recap what the features are:\n",
    "\n",
    "- `median_income` is the median income in that block group\n",
    "- `pct_broadband` is the percentage with access to broadband\n",
    "- `pct_white` and `pct_black` are the percentage of each Census block group's population that is White and Black\n",
    "- `pop_density` is the density of blockgroup (e.g. in units of population per square kilometer) \n",
    "- `max_speed` is the maximum contractual downstream speed offered by any provider in each Census block group\n",
    "- `num_isp` is the number of unique ISPs that offer service in each Census block group\n",
    "- `num_broadband` is the number of unique ISPs that offer service at or above 25 Mbps downstream and 3 Mbps upstream in each Census block group (This is the FCC's definition of broadband Internet access, which you can read about more in the 2019 broadband deployment report)\n",
    "\n",
    "First, we load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../data/fcc_acs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Now, train a decision tree classifier to predict if a Census block group has broadband Internet access or not (i.e., at least one ISP that offers service at or above 25 Mbps downstream and 3 Mbps upstream). Tune your classifier with a hyperparameter grid and use k-fold cross validation. \n",
    "\n",
    "- Divide the dataset into training and testing, with a .2 test_size. Use random_state=0 for this, and when training your classifiers.\n",
    "- Use `random_state=0` when both splitting your data and training your classifiers.\n",
    "- Preprocess your data by imputing missing values with the mean of the column from the training set. Note that (as you saw in the previous assignment) an administrative code is used for some missing values in `median income`, and not NaN.\n",
    "- Given the size of this dataset, it will be sufficient to forgo the validation set and only separate your data into training and testing sets (i.e., use `sklearn.model_selection.test_train_split`)\n",
    "- Your classifier should use the following features:\n",
    "    - The ACS population characteristics described above (percentage of population that is White and Black, median income, population density)\n",
    "    - The number of ISPs serving that block group\n",
    "- Tune your classifier with a hyperparameter grid, using the following hyperparameter options:\n",
    "    - `criterion`: `gini` or `entropy`\n",
    "    - `max_depth`: 1, 3, 5\n",
    "    - `min_samples_split`: 2, 5, 10\n",
    "- Use K-fold cross validation with `k = 10`, using accuracy as your scoring metric.\n",
    "- For scoring, use accuracy, precision, and recall.\n",
    "\n",
    "What is the best mean test accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, divide your data into train and test sets, using random_state = 0 and a split of .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, begin to preprocess your data:\n",
    "\n",
    "# First, for median income, replace the administrative code that indicates missing data with NaN\n",
    "# Then, fill the missing values in each column with the mean of that column from the training set\n",
    "# Hints: after you do this, the mean of the median_income column in the training set should be about 60906;\n",
    "# in the testing set, the median_income column should have a mean of about 57194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final preprocessing step will be generating your target: you're predicting whether or not a block has broadband,\n",
    "# not the number of unique ISPs with broadband, which is what your data currently has.\n",
    "# Create a has_broadband column in the train and test sets.\n",
    "# Assign 1 to it if num_broadband is greater than 0, and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, use GridSearch to tune a decision tree classifier, with has_broadband as the target\n",
    "# Use 10 folds and the parameters above\n",
    "# When creating your decision tree, remember to use random_state=0\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "k = 10 # Set the number of folds\n",
    "\n",
    "params = { #Set the parameters to use for GridSearch\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'max_depth': (1, 3, 5),\n",
    "    'min_samples_split': (2, 5, 10)\n",
    "}\n",
    "\n",
    "# Your code to conduct your GridSearch here\n",
    "\n",
    "# Afterwards, you can access the best score with .best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "You should find that one of the best trees used for its splitting criterion \"entropy\", a max depth of 3, and a minimum sample split of 10.\n",
    "\n",
    "For a tree trained with those parameters, what is the most important feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a tree with the parameters above\n",
    "# Then, find the feature with the highest importance\n",
    "# You can access feature importances with my.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Now, you want to plot the confusion matrix for the test set for the tree with the hyperparameters above (split on entropy, max depth of 3, and min samples split of 10). Which of the following matrices corresponds to that tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confusion matrix for the tree with the hyperparameters above\n",
    "# You can use sklearn.metrics.plot_confusion_matrix. It will take as parameters the tree, the test x, and test y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What is the F1 score for this confusion matrix?\n",
    "\n",
    "<img src=\"../assets/man_calc.PNG\">\n",
    "\n",
    "Hint:\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 = $\\frac{2 * precision * recall}{precision + recall} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This asks you to calculate these metrics manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Next, train a Random Forest classifier to predict broadband Internet deployment. As before, tune your classifier with a hyperparameter grid and use k-fold cross validation.\n",
    "\n",
    "- Use sckit-learn's `GridSearchCV` function\n",
    "- Continue to use `random_state=0` when both splitting your data and training your classifiers.\n",
    "- Given the size of this dataset, it will be sufficient to forgo the validation set and only separate your data into training and testing sets (i.e., use `sklearn.model_selection.test_train_split`)\n",
    "- Your classifier should use the following features:\n",
    "    - The ACS population characteristics from Section 2.2 (percentage of population that is White and Black, median income, population density)\n",
    "    - The number of ISPs serving that block group\n",
    "- Tune your classifier with a hyperparameter grid, using the following hyperparameter options (note the differences for Random Forests):\n",
    "    - `n_estimators`: 1, 10, 20 \n",
    "    - `criterion`: `gini` or `entropy`\n",
    "    - `max_depth`: 1, 3, 5\n",
    "    - `min_samples_split`: 2, 5, 10\n",
    "- Use K-fold cross validation with `k = 10`, using accuracy as your scoring metric.\n",
    "\n",
    "Note: In practice, you may want to use thousands of estimators. We only try a few here to save time--and this may still take a few minutes. You may want to test your code first on a sample of the larger dataset.\n",
    "\n",
    "Which combination of hyperparameters gives the best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your process will be very similar to when you did Grid Search on a decision tree\n",
    "k = 10 # Set the number of folds\n",
    "params = { # Set the hyperparameters\n",
    "    'n_estimators': (5,50,100),\n",
    "    'criterion': ('gini', 'entropy'),\n",
    "    'max_depth': (1, 3, 5),\n",
    "    'min_samples_split': (2, 5, 10)\n",
    "}\n",
    "\n",
    "# Your code to conduct your Grid Search here\n",
    "\n",
    "# Finally, find the parameters that produced the best score (you can access with .best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "So far, we've been using GridSearch and Cross-Validation (with GridSearchCV) to find the best models. Internally, this method is running \"predict\" on each validation fold, computing the evaluation metrics (accuracy, precision, etc.) for each fold and returning the average for the metrics. Predict uses 0.5 as the default classification threshold, meaning every observation with a probability score above 0.5 is classified as True.\n",
    "\n",
    "In many cases, this is not ideal. Instead, we might want to choose a classification threshold ourselves, rather than use the default 0.5 cutoff. The threshold might be a fixed value (say every observation with a probability above 0.7 receives a predicted label of True). Alternatively, the threshold might be based on a percentage of the total observations. Which approach to choose will depend on your problem. Here, we will look only at the former.\n",
    "\n",
    "One way to approach finding a good fixed threshold is to look at how different thresholds affect your evaluation metrics. You may want to choose one that optimizes for precision or recall, or one that is in a 'sweet spot' that balances nicely precision and recall.\n",
    "\n",
    "One of the best Random Forest classifiers from the Grid Search in the previous question had the following parameters:\n",
    "{'criterion': 'gini',\n",
    " 'max_depth': 5,\n",
    " 'min_samples_split': 5,\n",
    " 'n_estimators': 100}\n",
    "\n",
    "Train a Random Forest classifier with the parameters above on your training sets, once again using `random_state=0`. Then plot a precision-recall curve using the test sets. You can use sklearn.metrics.plot_precision_recall_curve.\n",
    "\n",
    "Which combination of precision and recall seem like a good balance that, with the right threshold, would be achievable by your Random Forest classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train a Random Forest Classifier with the parameters above\n",
    "# Then, plot a precision recall curve. We import the library for you below\n",
    "from sklearn.metrics import plot_precision_recall_curve \n",
    "\n",
    "# Eyeball where we get the best precision and the best recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
